{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 12, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Intro to ML, contd. - Distances in Feature Space and Dimensionality Reduction\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Questions about HW8 </center>\n",
    "    \n",
    "### The basic idea is straightforward:\n",
    "\n",
    "### You can't model populations without accounting for how your sample was selected. \n",
    "### You can't model the selection effects without modeling the data accquistion process.\n",
    "### This is conceptually straightforward but the implementation can get complex - take it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "#### Supervised learning needs labelled data\n",
    "- Every algorithm will take features and labels for a training sample\n",
    "- and optimize some metric you choose\n",
    "- which results in a classifier with hyperparameters that are conditioned on the training sample\n",
    "- that can then be applied to a test sample for prediction \n",
    "\n",
    "#### Tree methods are easy to interpret and are really useful in ensembles (random forests/boosted decision trees) \n",
    "- by no means the only, or even the best choice of ML algorithm (single decision trees are almost completely useless)\n",
    "- if you are fortunate enough to have labelled data, decision tree ensembles like random forests are a good place to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"decision_tree.png\" width=\"50%\">\n",
    "\n",
    "### Shortcomings:\n",
    "- Can't deal well with missing data\n",
    "- When dealing with predicting a categorical variable as output (i.e. class) it's hard to get $d\\mathrm{class}/d\\mathrm{parameter}$ so hard to incorporate uncertainty \n",
    "- Can be hard to construct training and testing sets that are representative - training data with labels often has different selection effects from test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But the biggest single issue - what do we do if we *don't* have labelled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Something like entropy/Gini purity doesn't work because you don't have labels \n",
    "- in the cartoon, you don't know which points are (+) and (-)\n",
    "\n",
    "<img src=\"entropy_twoclass.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Without labels, you have no recourse but to find structure in the data itself\n",
    "\n",
    "### Goal is still the same - partition the data into maximally homogenous and maximally distinguished subsets - except now without labels\n",
    "\n",
    "<img src=\"cluster_vs_class.png\">\n",
    "\n",
    "### This is usually a more common problem in astrophysics - you have to get lucky to start with labelled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ALSO, just because you do have labels, doesn't mean you should believe them:\n",
    "To err is human.\n",
    "\n",
    "<img src=\"Galileo_Neptune.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Just as before, clustering requires some notion of a distance/metric\n",
    "\n",
    "- members of the cluster should be similar/close to each other\n",
    "- samples outside the cluster should be dissimilar/separated from cluster members\n",
    "\n",
    "### Optimal clustering therefore depends on how you define this measure of similarity/distance and what the purpose of the clustering is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ideally, we want clustering approaches to have \n",
    "\n",
    "- scalability (remember, we're dealing with big data i.e. high dimensional spaces with lots of samples)\n",
    "    - spoiler: Naieve clustering approaches are NP hard\n",
    "- ability to deal with different types of attributes\n",
    "- ability to discover clusters with arbitrary shapes\n",
    "\n",
    "<img src=\"HR_diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- insensitivity to noise and outliers \n",
    "- insensitivity to order\n",
    "- flexibility to incorporate constraints/priors\n",
    "- be interpretable\n",
    "\n",
    "So we'll have to think about more abstract notions of **distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cluster_distance.png\">\n",
    "\n",
    "This map shows a spatial separation, but the feature space is 50-year mean monthly temperature, 50-year mean monthly precipitation, elevation, total plant-available water content of soil, total organic matter in soil, and total Kjeldahl soil nitrogen (i.e. the sum of ammonia + ammonium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances - continuous variables on ${\\rm I\\!R}$\\:\n",
    "\n",
    "### The Minkowski family:\n",
    "\n",
    "## $$ D_p(i,j) =  \\left({\\sum_{k \\mathrm{\\, features}}|x_{i,k} - x_{j,k}|^p} \\right)^{1/p} $$\n",
    "\n",
    "Properties:\n",
    "- $D(i, j) >= 0$ (see the absolute value symbols)\n",
    "- $D(i, i) = 0$ (so similar things are close)\n",
    "- $D(i, j) = D(j, i)$ (so order doesn't matter)\n",
    "- $D(i, j) <= D(i, k) + D(k, j)$ (triangle inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"minkowski1.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"minkowski2.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NOTE: NOT ALL CONTINUOUS VARIABLES ARE DEFINED ON ${\\rm I\\!R}$:\n",
    "<img src=\"minkowski_vs_gcd.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OK, but some features aren't continuous at all\n",
    "\n",
    "- e.g. you either have a spectral line or you don't\n",
    "- a galaxy has some integer number of nearby companions \n",
    "- many variables affecting house prices\n",
    "\n",
    "In this case, you can use the presence or absence of data to define a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Even categorical variables can be described as the presence or absence of data: [One hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "<img src=\"onehot.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cat1.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cat2.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### This is closely connected to something you've seen before\n",
    "\n",
    "<img src=\"cat3.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK, so we've now got a way to get distances between our samples - regardless of if features are continuous/categorical \n",
    "\n",
    "### But do those distances actually measure something useful vis-a-vis sample similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data can (and generally does) have covariance:\n",
    "\n",
    "Attributes are not independent - they aren't telling you different things\n",
    "\n",
    "[This can happen even with completely unrelated quantities:](https://www.tylervigen.com/spurious-correlations)\n",
    "\n",
    "<img src=\"spurious_corr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"corr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Aquisition\n",
    "\n",
    "From the SDSS Sky Server we've downloaded two types of photometry (aperature and petrosian), corrected for extinction, for a number of sources with redshifts. Here's the SQL for an example query, that gets us 10000 example quasars:\n",
    "\n",
    "<font color=\"blue\" size=2>\n",
    "<pre>SELECT *,dered_u - mag_u AS diff_u, dered_g - mag_g AS diff_g, dered_r - mag_r AS diff_g, dered_i - mag_i AS diff_i, dered_z - mag_z AS diff_z from\n",
    "(SELECT top 10000\n",
    "objid, ra, dec, dered_u,dered_g,dered_r,dered_i,dered_z,psfmag_u-extinction_u AS mag_u,\n",
    "psfmag_g-extinction_g AS mag_g, psfmag_r-extinction_r AS mag_r, psfmag_i-extinction_i AS mag_i,psfmag_z-extinction_z AS mag_z,z AS spec_z,dered_u - dered_g AS u_g_color, \n",
    "dered_g - dered_r AS g_r_color,dered_r - dered_i AS r_i_color,dered_i - dered_z AS i_z_color,class\n",
    "FROM SpecPhoto \n",
    "WHERE (class = 'QSO') ) as sp\n",
    " </pre>\n",
    "</font>\n",
    "\n",
    "We can do the same for 'STAR's and 'GALAXY's as well. These data are all provided in the `data` folder.\n",
    "\n",
    "Credit: Phil Marshall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND THIS\n",
    "qsos = pd.read_csv(\"qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"spec_z\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\"])\n",
    "\n",
    "# Clean out extreme colors and bad magnitudes:\n",
    "qsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "qso_redshifts = qsos[\"spec_z\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "qso_features = copy.copy(qsos)\n",
    "del qso_features[\"spec_z\"]\n",
    "qso_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND THIS\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at z=2.5 just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(qso_redshifts.values), vmax=2.5)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.plotting.scatter_matrix(qso_features, alpha=0.1, figsize=[15,15], c=m.to_rgba(qso_redshifts.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND THIS\n",
    "corrMatrix = qso_features.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# This shouldn't be shocking - after all there is just an underlying SED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction##\n",
    "\n",
    "Fitting and overfitting get worse with ''curse of dimensionality'' Bellman 1961\n",
    "\n",
    "Think about a hypersphere. Its volume is  given by\n",
    "\n",
    "\\begin{equation}\n",
    "  V_D(r) = \\frac{2r^D\\pi^{D/2}}{D\\  \\Gamma(D/2)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Gamma(z)$ is the complete gamma function, $D$ is the dimension, and $r$ the radius of the sphere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you populated a hypercube of size $2r$ how much data would be enclosed by the hypersphere\n",
    "- as $D$ increases the fractional volume enclosed by the hypersphere goes to 0! \n",
    "\n",
    "For example: the SDSS comprises a sample of 357 million sources. \n",
    "- each source has 448 measured attributes\n",
    "- selecting just 30 (e.g., magnitude, size..) and normalizing the data range $-1$ to $1$\n",
    "\n",
    "probability of having one of the 357 million sources reside within a unit hypersphere 1 in 1.4$\\times 10^5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def unitVolume(dimension, radius=1.):\n",
    "    return 2*(radius**dimension *np.pi**(dimension/2.))/(dimension*sp.gamma(dimension/2.))\n",
    "\n",
    "dim = np.linspace(1,100)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(dim,unitVolume(dim)/2.**dim)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('$Dimension$')\n",
    "ax.set_ylabel('$Volume$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "D=30;unitVolume(D)/(2.**D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can we do to reduce the dimensions##\n",
    "\n",
    "**Principal component analysis or the Karhunen-Loeve transform**\n",
    "\n",
    "The first refuge of a scoundrel...\n",
    "\n",
    "<img width=\"500\" src=\"pca-scatter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Points are  correlated along a particular direction which doesn't align with the initial choice of axes. \n",
    "- we should rotate our axes to align with this correlation. \n",
    "- rotation preserves the relative ordering of data\n",
    "\n",
    "Choose  rotation to maximize the ability to discriminate between the data points\n",
    "*   first axis, or <u>principal component</u>, is direction of maximal variance\n",
    "*   second principal component is orthogonal to the first component and maximizes the residual variance\n",
    "*   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of principal component analyses##\n",
    "\n",
    "Set of data $X$: $N$ observations by $K$ measurements\n",
    "\n",
    "Center data by subtracting the mean \n",
    "\n",
    "The covariance is\n",
    "\n",
    ">$ \n",
    "C_X=\\frac{1}{N-1}X^TX,\n",
    "$\n",
    "\n",
    "$N-1$ as the sample covariance matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a projection, $R$,  aligned with the directions of maximal variance ($Y= X R$) with covariance \n",
    "\n",
    ">$\n",
    "C_{Y} = R^T X^T X R = R^T C_X R\n",
    "$\n",
    "\n",
    "Derive  principal component by maximizing its variance (using Lagrange multipliers and constraint)\n",
    "\n",
    "> $\n",
    "\\phi(r_1,\\lambda_1) = r_1^TC_X r_1 - \\lambda_1(r_1^Tr_1-1).\n",
    "$\n",
    "\n",
    "derivative of $\\phi(r_1,\\lambda)$ with respect to $r_1$ set to 0\n",
    "\n",
    "> $\n",
    "C_Xr_1 - \\lambda_1 r_1 = 0.\n",
    "$\n",
    "\n",
    "$\\lambda_1$ is the root of the equation $\\det(C_X -\n",
    "\\lambda_1 {\\bf I})=0$ and the largest eigenvalue\n",
    "\n",
    ">$\n",
    "\\lambda_1 =  r_1^T C_X r_1\n",
    "$\n",
    "\n",
    "Other  principal components  derived by\n",
    "applying additional constraint that components are uncorrelated (e.g., $r^T_2 C_X r_1 = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian mulitpliers\n",
    "<img width=\"500\" src=\"LagrangeMultipliers2D.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation of principal components##\n",
    "\n",
    "Common approach is eigenvalue decomposition of the covariance or correlation matrix,\n",
    "or singular value decomposition (SVD) of the data matrix\n",
    "\n",
    "** SVD given by**\n",
    "\n",
    ">$\n",
    "U \\Sigma V^T = \\frac{1}{\\sqrt{N - 1}} X,\n",
    "$\n",
    "\n",
    "columns of $U$ are  _left-singular vectors_\n",
    "\n",
    "columns of $V$ are the _right-singular vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The columns of $U$ and $V$ form orthonormal bases ($U^TU = V^TV = I$)\n",
    "\n",
    "Covariance matrix is\n",
    "\n",
    "> $\n",
    "\\begin{eqnarray}\n",
    "  C_X &=& \\left[\\frac{1}{\\sqrt{N - 1}}X\\right]^T \\left[\\frac{1}{\\sqrt{N - 1}}X\\right]\\nonumber\\\\\n",
    "      &=& V \\Sigma U^T U \\Sigma V^T\\nonumber\\\\\n",
    "      &=& V \\Sigma^2 V^T.\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "right singular vectors $V$ are the principal components. We can calculate principal components from the SVD of $X$ - we dont need $C_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"500\" src=\"fig_svd_visual_1.png\">\n",
    "Singular value decomposition (SVD) can factorize an N x K matrix into $U \\Sigma V^T$. There are different conventions for computing the SVD in the literature, and this figure illustrates the convention used in this text. The matrix of singular values $\\Sigma$ is always a square matrix of size [R x R] where R = min(N, K). The shape of the resulting U and V matrices depends on whether N or K is larger. The columns of the matrix U are called the left-singular vectors, and the columns of the matrix V are called the right-singular vectors. The columns are orthonormal bases, and satisfy $U^T U = V^T V = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing data for PCA##\n",
    "\n",
    "- Center data by subtracting the mean of each dimension\n",
    "- For heterogeneous data (e.g., galaxy shape and flux) divide by  variance (whitening). **why?**\n",
    "- For spectra or images normalize each row so integrated flux of each object is one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Compute PCA\n",
    "def compute_PCA(n_components=5):\n",
    "#    np.random.seed(500)\n",
    "    nrows = 500\n",
    "    ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "    \n",
    "    spec_mean = spectra[ind].mean(0)\n",
    "#    spec_mean = spectra[:50].mean(0)\n",
    "\n",
    "    # PCA: use randomized PCA for speed\n",
    "    pca = PCA(n_components - 1)\n",
    "    pca.fit(spectra[ind])\n",
    "    pca_comp = np.vstack([spec_mean,\n",
    "                          pca.components_])\n",
    "    evals = pca.explained_variance_ratio_\n",
    "\n",
    "    return pca_comp, evals\n",
    "\n",
    "n_components = 5\n",
    "decompositions, evals = compute_PCA(n_components)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = 'PCA components'\n",
    "\n",
    "for j in range(n_components):\n",
    "    ax = fig.add_subplot(n_components, 2, 2*j+2)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax.plot(wavelengths, decompositions[j], '-k', lw=1)\n",
    "\n",
    "    # plot zero line\n",
    "    xlim = [3000, 7999]\n",
    "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax2 = fig.add_subplot(n_components, 2, 2*j+1)\n",
    "    ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax2.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax2.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax2.plot(wavelengths, spectra[j], '-k', lw=1)\n",
    "    \n",
    "    # plot zero line\n",
    "    ax2.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    if j == 0:\n",
    "        ax.set_title(titles, fontsize='medium')\n",
    "\n",
    "    if j == 0:\n",
    "        label = 'mean'\n",
    "    else:\n",
    "        label = 'component %i' % j\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax2.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax.text(0.02, 0.95, label, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='w', fc='w'),\n",
    "            fontsize='small')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the PCA##\n",
    "\n",
    "Reconstruction of spectrum, ${x}(k)$, from the\n",
    "eigenvectors, ${e}_i(k)$ \n",
    "\n",
    ">$ \\begin{equation}\n",
    "  {x}_i(k) = {\\mu}(k) + \\sum_j^R \\theta_{ij} {e}_j(k),\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Truncating this expansion (i.e., $r<R$)\n",
    "\n",
    ">$\\begin{equation}\n",
    "{x}_i(k) = {\\mu}(k) + \\sum_i^{r<R} \\theta_i {e}_i(k),\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "- eigenvectors ordered by their associated eigenvalues \n",
    "- eigenvalues reflect variance  within each eigenvector (sum of the eigenvalues is total variance of the system).\n",
    "- project a each spectrum onto these first few eigenspectra is a compression of the data \n",
    "\n",
    "This is the sense in which PCA gives for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute PCA components\n",
    "\n",
    "# Eigenvalues can be computed using PCA as in the commented code below:\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA()\n",
    "#pca.fit(spectra)\n",
    "#evals = pca.explained_variance_ratio_\n",
    "#evals_cs = evals.cumsum()\n",
    "\n",
    "#  because the spectra have been reconstructed from masked values, this\n",
    "#  is not exactly correct in this case: we'll use the values computed\n",
    "#  in the file compute_sdss_pca.py\n",
    "evals = data['evals'] ** 2\n",
    "evals_cs = evals.cumsum()\n",
    "evals_cs /= evals_cs[-1]\n",
    "evecs = data['evecs']\n",
    "spec_mean = spectra.mean(0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Find the coefficients of a particular spectrum\n",
    "spec = spectra[1]\n",
    "coeff = np.dot(evecs, spec - spec_mean)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the sequence of reconstructions\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "for i, n in enumerate([0, 4, 8, 20]):\n",
    "    ax = fig.add_subplot(411 + i)\n",
    "    ax.plot(wavelengths, spec, '-', c='gray')\n",
    "    ax.plot(wavelengths, spec_mean + np.dot(coeff[:n], evecs[:n]), '-k')\n",
    "\n",
    "    if i < 3:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_ylim(-2, 21)\n",
    "    ax.set_ylabel('flux')\n",
    "\n",
    "    if n == 0:\n",
    "        text = \"mean\"\n",
    "    elif n == 1:\n",
    "        text = \"mean + 1 component\\n\"\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "    else:\n",
    "        text = \"mean + %i components\\n\" % n\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "\n",
    "    ax.text(0.01, 0.95, text, ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "fig.axes[-1].set_xlabel(r'${\\rm wavelength\\ (\\AA)}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- k-means\n",
    "- GMMs\n",
    "- EM\n",
    "- hierarchical clustering -> trees\n",
    "- DBScan"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
